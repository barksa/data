{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Several Different Prompting Methodologies (via SOLAR 10.7B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee43b7f51b1488ab50f088b7083b1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b782d99dc0415a905f5ff62840306a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3226b52168c54d1faa05605a6038f44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2dbb724d384da28c350d969c25bcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d921e23e7ac49d180f889afa0957a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/35.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945f1abcce9647cf834c04ff247bf0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcf5d08dd544c5382f815f8114875de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00005.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304c7efa69da4d229bd4d0c056a555a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913c27b130a24b738adaf97fc1bb1c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4d8df1cbb04e00a77927d9b0ed803f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a452d8ff3334f8884f2e153ca70c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00005.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be7b33f077541a4aefcf64d3d2f1c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ffda8a9b524b988102b974f76d9b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Upstage/SOLAR-10.7B-Instruct-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Upstage/SOLAR-10.7B-Instruct-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot + Instruction Prompting\n",
    "- Simply feed the task text to the model and ask for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### User:\n",
      "Choose 3 from the following descriptions that match the name 'Snowflake': Playful, Gentle, Majestic, Curious, Loyal, Witty, Mysterious, Adventurous, Elegant, Charming\n",
      "\n",
      "### Assistant:\n",
      "1. Playful\n",
      "2. Gentle\n",
      "3. Majestic</s>\n"
     ]
    }
   ],
   "source": [
    "zero_shot = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Choose 3 from the following descriptions that match the name 'Snowflake': Playful, Gentle, Majestic, Curious, Loyal, Witty, Mysterious, Adventurous, Elegant, Charming\",\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    zero_shot, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, use_cache=True, max_length=4096)\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot + Instruction Prompting\n",
    "- Feed a set of high-quality demonstrations, each consisting of both input and desired output, on the target task.\n",
    "- Likely to feed similar names : use k-NN in the embedding space ([Liu et al., 2021](https://arxiv.org/abs/2101.06804))\n",
    "- Constructing good example set might be another task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Based on the examples below, choose 3 from the following descriptions that match the name 'Snowflake': Playful, Gentle, Majestic, Curious, Loyal, Witty, Mysterious, Adventurous, Elegant, Charming\\n\\nExample 1:\\nName: Snowwhite\\nChosen Descriptions: Playful, Gentle, Majestic\\n\\nExample 2:\\nName: Snowy\\nChosen Descriptions: Curious, Loyal, Witty\",\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    few_shot, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, use_cache=True, max_length=4096) \n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot + Instruction Prompting (alternative)\n",
    "__*Using different tasks as example sets.*__\n",
    "- Repeating the same task as an example is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_alt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Based on the examples below, choose 3 from the following descriptions that match the name 'Snowflake': Playful, Gentle, Majestic, Curious, Loyal, Witty, Mysterious, Adventurous, Elegant, Charming\\n\\nExample 1:\\nName: Snow\\nChosen Descriptions: White, Bright, Romantic\\n\\nExample 2:\\nName: Winter\\nChosen Descriptions: Frosty, Cold, Serene\\n\\nExample 3:\\nName: Christmas\\nChosen Descriptions: Romantic, Cold, Serene\",\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    few_shot_alt, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, use_cache=True, max_length=4096) \n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Consistency Sampling : [Wang et al., 2022a](https://arxiv.org/pdf/2203.11171.pdf)\n",
    "- Sample diverse answers through different reasoning paths, then apply majority vote.\n",
    "- $temperature > 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_sampling = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Choose 3 from the following descriptions that match the name 'Snowflake': Playful, Gentle, Majestic, Curious, Loyal, Witty, Mysterious, Adventurous, Elegant, Charming. Now, using self-consistency sampling, describe a fictional or idealized version of Snowflake, incorporating those chosen characteristics into a cohesive and vivid portrayal.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "sc_sampling_alt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"By the method below, choose 3 from the following descriptions that match the name 'Snowflake': Playful, Gentle, Majestic, Curious, Loyal, Witty, Mysterious, Adventurous, Elegant, Charming\\n\\nFirst, pick the very best description that match name about 20 times, use different reasoning pathway each.\\n\\nThen show me the top three most frequently chosen descriptions. Description is not needed.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    sc_sampling, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, use_cache=True, max_length=4096)\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Citation :__\n",
    "[Lilian Weng, 2023](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
